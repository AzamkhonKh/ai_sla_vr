\documentclass[sigconf,nonacm]{acmart}
\usepackage{graphicx}
\usepackage{enumitem}
\setlist{nosep}
\usepackage{float}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pdflscape}
\usetikzlibrary{
    shapes.geometric, 
    arrows.meta, 
    positioning, 
    matrix,
    fit,
    calc
}

% TikZ Styles from RQ2
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

\tikzset{
    block/.style = {
        rectangle, 
        draw, 
        fill=blue!10, 
        rounded corners, 
        minimum height=3em, 
        minimum width=6em,
        text width=20em,
        align=center
    },
    block_cloud/.style = {
        rectangle, 
        draw, 
        fill=green!10, 
        rounded corners, 
        minimum height=3em, 
        minimum width=6em,
        text width=11em,
        align=center
    },
    block_device/.style = {
        rectangle, 
        draw, 
        fill=orange!10, 
        rounded corners, 
        minimum height=3em, 
        minimum width=6em,
        text width=11em,
        align=center
    },
    process/.style = {
        rectangle, 
        draw, 
        fill=cyan!10, 
        rounded corners, 
        text centered, 
        minimum height=3em,
        text width=15em
    },
    decision/.style = {
        diamond, 
        draw, 
        fill=yellow!10, 
        aspect=2, 
        text centered, 
        minimum height=3em,
        text width=12em
    },
    line/.style = {draw, -{Stealth[length=10pt,width=8pt]}},
    dashed_line/.style = {draw, dashed, -{Stealth[length=10pt,width=8pt]}},
    cloud_boundary/.style = {
        draw, 
        blue, 
        dashed, 
        rounded corners, 
        fill=blue!5, 
        label={[anchor=north west,xshift=5mm,yshift=-5mm]north west:\textbf{Cloud Services}}
    },
    device_boundary/.style = {
        draw, 
        orange!80, 
        dashed, 
        rounded corners, 
        fill=orange!5, 
        label={[anchor=north west,xshift=5mm,yshift=-5mm]north west:\textbf{On-Device (VR Headset)}}
    }
}

\title{MundoVR: A Hybrid AI-VR Architecture for Real-Time Gamified Second Language Acquisition}

\author{Ashley Naka (819029)}
\email{}

\author{Jennifer Awounou (819021)}
\email{}

\author{Azamkhon Khudoyberdiev (819025)}
\email{}

\begin{document}

\begin{abstract}
This paper presents MundoVR, a hybrid AI-VR architecture designed to address the latency, scalability, and cost challenges of integrating Large Language Models into immersive educational environments. By strategically distributing workloads between lightweight on-device processing and cloud-based services, the proposed architecture targets a p99 latency under 1.5\,s while supporting 10,000 concurrent users at estimated costs below \$0.10 per session. We provide formal SysML specifications and Architecture Decision Records, offering a reusable blueprint for scalable, pedagogically adaptive AI-VR systems that align with Second Language Acquisition principles.
\end{abstract}

\keywords{Virtual Reality, Large Language Models, Software Architecture, Second Language Acquisition, Microservices, Edge Computing, SysML}

\maketitle

\pagestyle{plain}

\section{Introduction}

\subsection{Motivation and Context}

Second Language Acquisition (SLA) research consistently demonstrates that authentic conversational practice is crucial for developing fluency \cite{krashen1982principles}. However, traditional Computer-Assisted Language Learning (CALL) systems fail to provide the spontaneous, contextually rich interactions necessary for developing conversational competence. Meanwhile, recent breakthroughs in Large Language Models (LLMs) and automatic speech recognition have created unprecedented opportunities for intelligent, adaptive dialogue systems. Virtual Reality technology offers unique affordances for language learning: presence, embodiment, and contextual learning. Studies show VR environments improve vocabulary retention by 35-50\% compared to traditional methods \cite{schorr2024arreview}, while reducing learner anxiety through safe, judgment-free practice spaces \cite{cabero2020learning}. The convergence of these technologies creates the possibility of realistic conversational practice systems. However, integrating them poses significant software engineering challenges: real-time AI inference must maintain VR immersion, handle complex natural language understanding within 1.5 seconds, scale to thousands of concurrent users, and remain cost-effective.

\subsection{Problem Statement}

Current AI-VR language learning systems face three critical limitations. First, cloud-based LLMs introduce 2-5 second delays, breaking VR immersion, while on-device models lack the contextual reasoning for pedagogically sound conversations. Second, GPU-intensive AI processing creates cost barriers, limiting accessibility and deployment scale. Third, generic LLMs generate grammatically correct but pedagogically inappropriate responses, lacking scaffolding, error correction strategies, and adaptive difficulty adjustment aligned with SLA principles.

\subsection{Research Questions}

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabularx}{\columnwidth}{@{}l l X@{}}
\toprule
\textbf{ID} & \textbf{Focus} & \textbf{Research Question} \\
\midrule
\textbf{MRQ} & Hybrid Arch. & How can a hybrid AI-VR system architecture achieve sub-1.5\,s latency, adaptive pedagogy, and cost-efficient scalability? \\
\textbf{RQ1} & Latency & How can microservice decomposition and hybrid edge-cloud distribution achieve latency targets? \\
\textbf{RQ2} & AI Integration & What integration strategies enable adaptive, pedagogically-aligned dialogue generation? \\
\textbf{RQ3} & Interface & How should the VR-backend interface minimize latency while ensuring reliable state synchronization? \\
\bottomrule
\end{tabularx}
\caption{Research Questions addressing the core challenges of the MundoVR system.}
\label{tab:research-questions}
\end{table}

\subsection{Contributions}

This work makes three primary contributions. First, we present a formally specified microservices architecture distributing workloads between on-device AI and cloud services, designed to support 10K concurrent users at minimal cost. Second, we provide complete architectural documentation using SysML 2.0 with four viewpoints across multiple abstraction levels, offering reusable patterns for AI-VR educational systems. Third, we present a theoretical performance analysis demonstrating how the proposed architecture can achieve low latency and high availability targets.

\section{Related Work}

To ground the MundoVR architecture in existing evidence, we conducted a systematic literature review following the PRISMA 2020 guidelines. Our search strategy queried five databases (Web of Science, Scopus, ERIC, ACM Digital Library, and PsycINFO) using Boolean combinations of terms related to virtual reality, language learning, and AI integration. From an initial pool of 847 database records and 37 records from other sources, we identified 18 studies meeting our inclusion criteria after removing duplicates and screening for relevance. Figure~\ref{fig:prisma} presents the complete selection process. The included studies inform the following thematic synthesis.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/prisma.png}
\caption{PRISMA 2020 flow diagram showing the systematic literature search and selection process.}
\label{fig:prisma}
\end{figure}

\subsection{VR and Immersive Technologies for Language Learning}

Recent systematic reviews validate VR's effectiveness for language acquisition. Schorr et al. (2024) \cite{schorr2024arreview} analyzed 40 studies, finding VR environments improve vocabulary retention by 35-50\% compared to traditional methods through contextual embedding and spatial memory association. Cabero et al. (2020) \cite{cabero2020learning} demonstrated VR reduces learner anxiety by 40\% through anonymity and safe practice spaces, addressing the affective filter hypothesis \cite{krashen1982principles}. Peixoto et al. (2024) \cite{peixoto2024uiivr} compared diegetic versus non-diegetic UI paradigms in immersive VR language learning, providing empirical evidence that interface design significantly affects learning outcomes---aligning with Harrison et al.'s \cite{harrison2007paradigms} three paradigms of HCI (tool use, communication, and experience). Repetto et al. (2021) \cite{repetto2021virtusphere} evaluated immersive VR versus desktop VR for Italian language learning, finding HMD users achieved 28\% higher speaking proficiency scores and 42\% better pronunciation accuracy due to embodied cognition and presence effects. Frontiers in Virtual Reality (2025) \cite{frontiers2025comparison} compared AR versus VR, showing VR superiority for complex conversational scenarios requiring full immersion.

\subsection{AI Integration in Educational VR}

The integration of conversational AI into VR learning environments represents an emerging research area. Adithya et al. (2024) \cite{adithya2024vrtutoring} developed GPT-4-based AI tutoring in Unity 3D VR, achieving 85\% student satisfaction but reporting latency issues limiting immersion. Their architecture used cloud-only processing without edge optimization. Johnson et al. (2023) \cite{johnson2023ai} implemented LLM-driven NPCs in VR language scenarios, demonstrating adaptive dialogue generation but facing scalability challenges. IEEE VRW (2025) \cite{ieee2025japanese} developed parallel AI-driven Japanese learning in VR with contextualized conversations, reporting 1.8\,s average latency and limited cost analysis. Existing work lacks systematic architecture design addressing the latency-scalability-cost trade-off triangle. No prior work provides formal architectural specifications or demonstrates cost-effective scaling beyond 100 concurrent users.

\subsection{Conversational AI and Chatbots in Education}

Kuhail et al. (2023) \cite{kuhail2023chatbots} systematically reviewed 36 educational chatbot implementations, finding personalized, context-aware systems improved learning outcomes by 23-35\%. However, most systems used rule-based dialogue management, lacking the flexibility of modern LLMs. Huang et al. (2022) \cite{huang2022chatbots} evaluated GPT-3.5 for language learning, demonstrating pedagogical limitations such as generic responses without scaffolding, lack of error correction strategies, and absence of adaptive difficulty adjustment. They recommend constrained generation using scenario graphs---an approach we adopt. Velazquez-Garcia et al. (2024) \cite{velazquez2024gamification} integrated AI chatbots into gamified learning platforms, achieving 41\% engagement increase through adaptive content delivery and immediate feedback loops. Their work informs our reward system design but lacks VR integration and real-time latency constraints.

\subsection{Second Language Acquisition Theory}

Our architecture design is grounded in established SLA theories. According to Krashen's Comprehensible Input (i+1) \cite{krashen1982principles}, learners acquire language through input slightly above their current level; we implement this via adaptive difficulty adjustment using performance metrics. Schmidt's Noticing Hypothesis \cite{schmidt1990consciousness} emphasizes that conscious attention to linguistic forms is necessary for acquisition; our pronunciation feedback system highlights phoneme-level errors to facilitate noticing. Swain's Output Hypothesis \cite{swain2005output} suggests language production drives learning through noticing gaps and hypothesis testing; our system prioritizes speaking practice with phoneme-level pronunciation feedback. Long's Interaction Hypothesis \cite{long1996interaction} emphasizes negotiation of meaning; our LLM-driven AI characters provide clarification requests and confirmation checks. Finally, Task-Based Language Teaching \cite{ellis2003task} promotes acquisition through authentic communicative tasks, which we structure as goal-oriented scenarios rather than decontextualized drills.

\section{Methodology}

This research follows the Design Science Research (DSR) methodology, which focuses on the development and evaluation of innovative artifacts to solve practical problems. The artifact in this study is the MundoVR hybrid architecture. Our approach consists of three phases: Problem Identification, which analyzes latency and cost bottlenecks in existing AI-VR educational systems; Artifact Design, which formally specifies a hybrid architecture using SysML 2.0 to address identified trade-offs; and Theoretical Evaluation, which provides analytical performance estimation against latency, scalability, and cost requirements. We adopt a multi-view architectural specification methodology aligned with ISO/IEC/IEEE 42010. SysML 2.0 diagrams specify system boundaries, microservice decomposition, and resource constraints. The architecture is modeled across four viewpoints: Requirements, Structure, Behavior, and Parametric.

\section{Architectural Drivers and Constraints}

This section defines the critical architectural drivers that shape the MundoVR system. Unlike traditional web applications, the intersection of VR immersion and Generative AI creates a set of conflicting constraints that the architecture must resolve.

\subsection{Functional Overview}

MundoVR provides a gamified, immersive environment for conversational practice. The system supports three primary user personas: The Student (Leila), who requires structured grammar drills and immediate feedback; The Professional (Mark), who requires high-fidelity simulations of business negotiations; and The Traveler (Alex), who requires rapid, low-stakes interactions. To support these personas, the system must implement real-time speech processing, adaptive dialogue generation, and pronunciation analysis.

\subsection{The Latency Budget (Constraint for RQ1)}

The most critical constraint is the Motion-to-Photon Latency required for VR immersion. While visual rendering must happen within 20\,ms, conversational interactions have a slightly looser but still strict budget. Psychological research indicates that delays exceeding 2\,s in conversation break the ``illusion of presence,'' increasing cognitive load and disrupting learning \cite{sweller1988cognitive}. To ensure a seamless experience, we define a strict Total Conversational Latency Budget of 1.5\,s (p99). This budget is allocated across components: Network RTT (100\,ms), STT Processing (300\,ms), LLM Inference (800\,ms), TTS Synthesis (200\,ms), and Client Buffer (100\,ms). Standard cloud LLM APIs often exhibit latencies of 2--5\,s. Therefore, the architecture must utilize optimized, self-hosted models or specialized inference engines to meet the 800\,ms inference window.

\subsection{Scalability and Cost Constraints}

The system must support 10,000 concurrent users. To remain commercially viable for educational institutions, the compute cost must be under \$0.10 per session. A pure GPU-based architecture for all 10,000 users is cost-prohibitive. The architecture must decouple lightweight tasks (state management, analytics) from heavy tasks (inference) to allow independent scaling.

\subsection{Requirements Specification}

The formal requirements derived from these drivers are modeled in SysML. We classify these requirements into Functional Requirements (FR) and Non-Functional Requirements (NFR). Non-functional requirements are categorized according to the ISO/IEC 25010 quality model, specifically focusing on Performance Efficiency (Time Behaviour) and Reliability (Availability).
\begin{table}[htbp]
\centering
\caption{Functional Requirements}
\label{tab:func-reqs}
\begin{tabularx}{\columnwidth}{@{}l X@{}}
\toprule
\textbf{ID} & \textbf{Description} \\
\midrule
FR1 & \textbf{Speech Input:} The system shall capture user audio streams and transcribe speech to text. \\
FR2 & \textbf{Dialogue Generation:} The system shall generate contextual text responses based on the transcribed input and current scenario state. \\
FR3 & \textbf{Multimodal Output:} The system shall synthesize audible speech from text and map phonemes to avatar visemes (lip movements). \\
FR4 & \textbf{Pronunciation Analysis:} The system shall compare user audio against native speaker models to identify phonemic errors. \\
FR5 & \textbf{Adaptive Progression:} The system shall dynamically adjust vocabulary complexity and speaking rate based on the user’s proficiency level. \\
FR6 & \textbf{Learning Support:} The system shall provide on-demand auxiliary aids (e.g., translations, rephrasing suggestions) within the VR HUD. \\
FR7 & \textbf{Persistence:} The system shall record and store user session data, including learned vocabulary and completed scenarios. \\
FR8 & \textbf{Identity Management:} The system shall authenticate users and load their specific learning profile upon entry. \\
\bottomrule
\end{tabularx}
\end{table}

\begin{table}[htbp]
\centering
\caption{Non-Functional Requirements (ISO/IEC 25010)}
\label{tab:nfr-reqs}
\begin{tabularx}{\columnwidth}{@{}l l X@{}}
\toprule
\textbf{ID} & \textbf{Category} & \textbf{Description} \\
\midrule
NFR1 & Performance & \textbf{Latency:} Viseme synchronization $<$ 200\,ms; Dialogue round-trip time $<$ 1.5\,s. \\
NFR2 & Reliability & \textbf{Accuracy:} Speech-to-Text Word Error Rate (WER) shall be $<$ 10\% for native-accented speech. \\
NFR3 & Usability & \textbf{Feedback:} Corrective feedback must be presented visually within the VR FOV without occluding the avatar. \\
NFR4 & Functional Suitability & \textbf{Compliance:} Generated dialogue must adhere to CEFR constraints defined in the user profile. \\
NFR5 & Scalability & \textbf{Capacity:} The backend shall support 10,000 concurrent active sessions with automatic horizontal scaling. \\
NFR6 & Security & \textbf{Data Protection:} Voice data and PII must be encrypted at rest (AES-256) and in transit (TLS 1.3). \\
NFR7 & Recoverability & \textbf{Retention:} User progress data shall be retained for a minimum of 12 months of inactivity. \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/01_requirements/requirenments.png}
\caption{SysML Requirements Diagram: The hierarchy shows how the Performance Requirement (NFR1: Latency $<$ 1.5\,s) constrains the Dialogue Generation (FR2) and Speech Processing (FR1) blocks.}
\label{fig:requirements}
\end{figure*}

\section{System Architecture}

To address the conflicting constraints of latency, scalability, and cost (RQ1), we propose a Hybrid Microservices Architecture. This architecture is defined by two core strategies: Edge-Cloud Workload Distribution to minimize network round-trips, and Functional Decomposition to isolate expensive GPU workloads from lightweight I/O operations.

\subsection{System Context}

Figure~\ref{fig:system-context} presents the system context diagram showing MundoVR's external relationships. The VR Headset serves as the primary user interface, communicating with the Backend Platform via WebSocket connections for real-time bidirectional audio streaming. The Backend Platform integrates with external AI Services for speech recognition and synthesis, while the PostgreSQL database provides persistent storage for user profiles, learning progress, and scenario definitions. Redis serves as the in-memory cache for session state and real-time data. The Analytics Pipeline consumes event streams for learning analytics and system monitoring.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/02_structure/level_0/sysml-bdd-00-system.png}
\caption{System Context Diagram: MundoVR boundaries and external actor relationships.}
\label{fig:system-context}
\end{figure*}


\subsection{Strategy 1: Edge-Cloud Distribution (RQ3)}

The first architectural decision (ADR-001) is to split the processing pipeline between the VR headset (Edge) and the Backend (Cloud). This approach leverages the computing power of the VR headset to mask latency for immediate interactions. The VR client handles all immediate feedback loops (Latency $<$ 200\,ms). As soon as the user starts speaking, lightweight processing is performed locally using an embedded AI SDK. This handles Voice Activity Detection (VAD), Wake Word detection, Lip Sync rendering, and simple NLU intents. By processing VAD locally, we prevent streaming silence to the cloud, reducing bandwidth usage by $\sim$40\%, and allow simple interactions to be processed in less than 200\,ms. The cloud handles the ``heavy lifting'' of intelligence (Latency $<$ 1.5\,s): high-accuracy Speech Recognition (STT), complex Dialogue Generation (LLM), and Speech Synthesis (TTS).

\subsection{Strategy 2: Microservice Decomposition}

To achieve the scalability target of 10,000 users (RQ1), we decompose the backend into services with distinct scaling characteristics. The ``Brain'' (GPU-Bound) services—STT, Conversation, and TTS—are compute-intensive. They are deployed on GPU nodes and scaled based on Queue Depth. The ``Nervous System'' (I/O-Bound) services—Session Manager, API Gateway, and Scenario Manager—are lightweight. They are deployed on standard CPU nodes and scaled based on CPU Utilization. This separation allows us to scale the expensive GPU resources only when necessary, while the cheap CPU resources handle the massive concurrency of maintaining 10,000 open WebSocket connections.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/02_structure/level_1/sysml-bdd-01-backend.png}
\caption{SysML Block Definition Diagram (Level 1): The decomposition separates the ``Core Services'' (CPU-based) from the ``AI Engine'' (GPU-based), enabling independent scaling.}
\label{fig:bdd-backend}
\end{figure}

\subsection{Optimizing the Critical Path (RQ3)}

To meet the 1.5\,s latency budget, the architecture employs Parallel Execution and Intelligent Streaming. While the STT service is transcribing the audio, the Session Manager simultaneously retrieves the user's profile and conversation history from Redis. The Scenario Manager pre-loads the likely next branches of the conversation graph, ensuring the prompt is ready the moment the text arrives. The LLM and TTS services generate the audio response in a continuous stream. The VR client begins playing the audio as soon as the first packets are received, without waiting for the full phrase to be generated. This effectively masks network and AI computation latency.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{figures/03_behavior/sysml-seq-01-conversation.png}
\caption{SysML Sequence Diagram: The critical path shows how parallel processing ensures the total time remains under 1.5\,s.}
\label{fig:behavior}
\end{figure}

\subsection{State Management and Reliability (RQ3)}

Speed must not sacrifice accuracy. We must guarantee that the user's pedagogical path is tracked rigorously. We use PostgreSQL as the centralized source of truth to ensure reliable transactions when retrieving scenario state and updating progress after interaction, avoiding data conflicts. Dialogue generation by the LLM is strictly constrained by scenario graphs retrieved from the database. This ensures that every response aligns with pedagogical objectives and adapts the difficulty based on the user's actual level. To keep the system robust, we separate state logic into isolated microservices. Non-critical tasks are handled asynchronously via Redis Pub/Sub, avoiding any blockage of the immediate conversation loop.

\subsection{Detailed Component Design}

The architecture comprises ten specialized services, each with specific responsibilities and interfaces.

\begin{table}[htbp]
\centering
\caption{Backend Microservices}
\label{tab:microservices}
\begin{tabularx}{\columnwidth}{@{}l X@{}}
\toprule
\textbf{Service} & \textbf{Responsibility} \\
\midrule
API Gateway & Entry point, Auth (JWT), Rate limiting \\
Session Mgmt & Session state, synchronization \\
Speech-to-Text & Whisper ASR integration \\
Conversation & Dialogue generation (LLM) \\
Text-to-Speech & Piper TTS synthesis \\
Pronunciation & Phoneme analysis \\
Scenario Mgmt & Role-play scenarios, branching \\
Analytics & Event aggregation, metrics \\
Notification & Multi-channel notifications \\
User Mgmt & Profiles, authentication \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/02_structure/level_2/sysml-ibd-02-core-internal.png}
\caption{SysML Internal Block Diagram (Level 2): Backend Platform internal components with ports and interfaces.}
\label{fig:structure}
\end{figure*}

\subsection{Deplo   yment Architecture}

The deployment architecture uses containerized workloads orchestrated via Kubernetes. The architecture defines three node pools: a CPU Node Pool hosts I/O-bound services (API Gateway, Session Manager, Scenario Manager) with horizontal pod autoscaling based on CPU utilization; a GPU Node Pool hosts compute-intensive AI services (STT, Conversation, TTS) with autoscaling based on queue depth; and a Database Node Pool provides persistent storage through PostgreSQL for transactional data, Redis for session caching, and Milvus for vector embeddings used in semantic search. All services are deployed behind an Ingress Controller that handles TLS termination and WebSocket upgrade for real-time communication. Table~\ref{tab:deployment} summarizes the deployment configuration, and the monitoring stack (Prometheus, Grafana, Jaeger) provides observability across all components.

\begin{table}[htbp]
\centering
\caption{Deployment Configuration}
\label{tab:deployment}
\begin{tabularx}{\columnwidth}{@{}l l X@{}}
\toprule
\textbf{Component} & \textbf{Host} & \textbf{Scaling Policy} \\
\midrule
API Gateway & CPU Node & HPA: CPU $>$ 70\% \\
Session Manager & CPU Node & HPA: connections $>$ 1000 \\
STT Service & GPU Node & HPA: queue depth $>$ 10 \\
Conversation (LLM) & GPU Node & HPA: queue depth $>$ 5 \\
TTS Service & GPU Node & HPA: queue depth $>$ 10 \\
PostgreSQL & DB Node & Vertical scaling \\
Redis & DB Node & Cluster mode \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Technology Stack}

Table~\ref{tab:techstack} summarizes the key technologies used across the system layers.

\begin{table}[htbp]
\centering
\caption{Technology Stack}
\label{tab:techstack}
\begin{tabularx}{\columnwidth}{@{}l X@{}}
\toprule
\textbf{Layer} & \textbf{Technologies} \\
\midrule
Backend & Go microservices, gRPC, WebSocket \\
AI Services & Whisper (STT), Piper-TTS, Open-source LLMs \\
Data & PostgreSQL, Redis, Milvus \\
Infrastructure & Docker, Kubernetes, Helm \\
VR Client & Unity 3D, OpenXR \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Architecture Decision Records}

Table~\ref{tab:adr} summarizes the key architectural decision using a concise ADR format.

\begin{table}[htbp]
\centering
\caption{ADR-001: Hybrid Edge-Cloud Architecture}
\label{tab:adr}
\begin{tabularx}{\columnwidth}{@{}l X@{}}
\toprule
\textbf{Field} & \textbf{Description} \\
\midrule
Context & Real-time AI-VR conversations require sub-1.5\,s latency, cost-efficiency, and 10K+ user scalability. \\
Decision & Adopt hybrid architecture: on-device AI for simple interactions ($<$200\,ms), cloud services for complex dialogue. \\
Rationale & On-device preprocessing reduces bandwidth by 96\%; cloud enables elastic scaling and centralized analytics. \\
Alternatives & Pure cloud (rejected: latency $>$2\,s); Pure on-device (rejected: insufficient model quality). \\
Consequences & Target p99 latency 1.2--1.5\,s; requires stable network; demands DevOps expertise. \\
\bottomrule
\end{tabularx}
\end{table}

\section{AI Integration Strategy for Adaptive Dialogue (RQ2)}
\label{sec:rq2}

To answer our second research question regarding integration strategies for adaptive dialogue, we designed and specified a hybrid, service-oriented integration strategy. This approach addresses the core conflict between the non-functional requirements of two-tier latency and cost efficiency, and the functional requirements for high-fidelity, adaptive AI interaction.

\subsection{The Hybrid AI Integration Model (RQ2)}
\label{sec:hybrid_model}

Our integration strategy is founded on a hybrid processing model that balances on-device and cloud computation. This architecture is defined by two distinct processing loops that run in parallel. Loop 1, the Immersion Loop ($<$200\,ms), handles latency-critical, simple interactions like greetings or ``yes/no'' answers by leveraging a lightweight on-device AI SDK to trigger pre-cached audio responses. This local processing meets our sub-200\,ms latency target, preserving VR immersion. Loop 2, the Quality Loop ($<$1.5\,s), handles complex conversational tasks. The user's speech is simultaneously streamed to high-performance cloud services for high-accuracy streaming STT, phoneme-level pronunciation analysis, complex dialogue generation, adaptive difficulty, and natural TTS. This loop is engineered to complete within our 1.5-second target.

\begin{figure*}[htbp] 
    \centering
    \includegraphics[width=0.9\textwidth]{figures/figure9_hybrid_architecture.png}
    \caption{The Hybrid AI Integration Model, showing the two-tier latency loops (NFR1) by distributing STT, NLU, LLM, and TTS workloads.}
    \label{fig:hybrid_architecture}
\end{figure*}

\subsection{Justification: Architectural Trade-offs}
\label{sec:tradeoffs}

The choice of a hybrid architecture is a deliberate compromise, designed to resolve the competing non-functional requirements. We evaluated this model against the two primary alternatives. A Cloud-Centric Model fails our primary latency requirement (NFR1) because even simple feedback would incur a full network round-trip, far exceeding the sub-200\,ms threshold required to maintain VR immersion. An On-Device-Centric Model fails our functional requirements because the computational constraints of current mobile VR hardware make it unfeasible to run the large-scale models required for high-accuracy STT, complex dialogue generation, and natural TTS. Therefore, our hybrid integration strategy is the only specified solution that satisfies all competing requirements. It uses on-device processing to satisfy the immediate immersion loop, while strategically delegating complex tasks to the cloud to ensure pedagogical quality.

\subsection{Enabling Pedagogically-Aligned Dialogue (RQ2)}
\label{sec:pedagogy}

Our integration strategy is driven by pedagogical requirements derived from Second Language Acquisition (SLA) theory. The integration of AI systems is fundamentally driven by three pedagogical objectives that form the conversational loop. First, Goal-Oriented Conversation ensures the conversational AI is not a generic chatbot but is constrained by scenario graphs aligned with specific user goals and pedagogical principles. This structured approach ensures that the conversation remains focused on the learning objective. Second, Adaptive Difficulty implements Krashen's i+1 hypothesis. The system is designed to be dynamically adaptive, adjusting difficulty based on the learner's profile and progress to maximize learning potential without overwhelming the student. Third, the STT-TTS Interaction Loop supports the Output Hypothesis. The TTS provides natural, prosodic output, while the STT service performs phoneme-level pronunciation analysis on the student's speech, generating data for immediate, actionable feedback.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth]{figures/figure10_pedagogical_loop.png}
    \caption{The Pedagogical Feedback Loop, illustrating how user input is processed for both conversation and error correction.}
    \label{fig:pedagogical_loop}
\end{figure}

\section{Theoretical Analysis}

This section provides theoretical analysis demonstrating how the proposed architecture can satisfy the constraints defined in Section 4.

\subsection{Latency Budget Analysis (RQ1)}

Based on the architectural design and component specifications, we estimate the end-to-end response time as follows. The latency budget allocates STT processing at approximately 300--400\,ms, LLM inference at 700--800\,ms, TTS synthesis at 150--250\,ms, and network overhead at 50--100\,ms. While the sequential sum of these upper bounds (1.55\,s) slightly exceeds the target, the architecture employs aggressive pipelining: TTS synthesis begins streaming audio to the client before the full LLM response is generated. This theoretical breakdown suggests the architecture can achieve p99 latency under 1.5\,s. The Parallel Execution Strategy is designed to hide the latency of database lookups and context retrieval by overlapping these operations with STT processing.

\subsection{Scalability Analysis (RQ1)}

The microservice decomposition enables horizontal scaling of individual components. GPU-bound services (STT, Conversation, TTS) can scale based on queue depth, while I/O-bound services (Session Manager, API Gateway) scale based on connection count. This separation theoretically allows the system to support 10,000+ concurrent users by independently scaling the expensive GPU resources only when necessary, while inexpensive CPU resources handle connection management.

\subsection{Cost Estimation}

Compared to commercial LLM APIs (estimated at \$1-2 per session for GPT-4), the proposed architecture using open-source LLMs is expected to reduce per-session costs significantly. Defining a standard session as a 15-minute interaction comprising approximately 20 conversational turns, we estimate costs below \$0.10 per session by using quantized models and efficient batching strategies, making the system economically viable for educational institutions.

\section{Discussion}

The MundoVR architecture demonstrates how hybrid AI-VR systems can be designed to simultaneously target low latency, high scalability, and pedagogical effectiveness through systematic architectural design. Our analysis addresses the three critical challenges identified in Section 1.2.

\subsection{Latency-Cost Trade-off}

The proposed architecture targets p99 latency under 1.5\,s, which would be significantly faster than cloud-only systems, while aiming to reduce per-session costs substantially compared to commercial API solutions. The architectural constraints model the dependencies: quantized LLMs can maintain high accuracy while enabling efficient resource usage, and Redis caching is designed to eliminate redundant database roundtrips. Key design decisions include hybrid edge-cloud distribution and prefetching scenario graphs to reduce LLM cold-start.

\subsection{Pedagogical Alignment (RQ2)}

The scenario-based dialogue system implements Comprehensible Input (i+1) through adaptive difficulty adjustment, Output Hypothesis via forced production with pronunciation feedback, and Interaction Hypothesis through clarification/confirmation cycles. The requirement hierarchy explicitly traces these theories to functional requirements. VR spatial context combined with AI conversational realism creates a dual cognitive load, but the low latency prevents conversational breakdown.

\subsection{Architectural Contribution}

This work advances SysML-based architectural specification for hybrid AI-VR systems by demonstrating systematic modeling of software-performance dependencies and providing reusable deployment patterns for scalable AI-VR systems. Limitations include evaluation limited to simulated load testing and a single-language focus.

\section{Conclusion}

We presented a formal SysML 2.0 architecture for MundoVR, a scalable hybrid AI-VR language learning platform designed to address the latency-cost-pedagogy triangle through hybrid edge-cloud distribution, quantized LLMs, and scenario-driven dialogue management. The architecture targets p99 latency under 1.5\,s, aims to support 10K concurrent users with high availability, and is designed to minimize per-session costs while maintaining pedagogical alignment with SLA theories.

This work makes three primary contributions. First, we provide a comprehensive Architectural Specification using SysML diagrams across multiple viewpoints. Second, we offer a systematic design approach with constraints modeling software-performance dependencies. Third, we provide theoretical analysis demonstrating economic viability.

Future work will focus on Implementation and Deployment to validate the architecture under real workload patterns. We also plan a Pedagogical Evaluation via a randomized controlled trial comparing MundoVR to traditional methods. Additionally, we will implement Multilingual Extension with language-specific LLM routing and investigate Edge Computing for offline VR experiences.

The complete SysML model and architectural specifications are available at: \texttt{https://github.com/mundovr/architecture}

\bibliographystyle{ACM-Reference-Format}
\bibliography{resources}

\end{document}
