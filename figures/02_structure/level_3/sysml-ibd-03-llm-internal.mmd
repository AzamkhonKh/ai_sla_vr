graph TB
    subgraph LLMService["<<part def>> LLM Service Internal Structure"]
        subgraph RequestHandler["<<part>> Request Handler"]
            InputValidator["Input Validator<br/>Max tokens: 8K"]
            QueueManager["Queue Manager<br/>Capacity: 100 req"]
        end
        
        subgraph ContextEngine["<<part>> Context Engine"]
            HistoryBuffer["History Buffer<br/>20 turns, 4K tokens"]
            PromptBuilder["Prompt Builder<br/>System + User + History"]
        end
        
        subgraph InferenceEngine["<<part>> Inference Engine"]
            TokenGen["Token Generator<br/>Rate: 50 tokens/s"]
            Sampler["Sampler<br/>Top-p: 0.95, Temp: 0.7"]
            BatchProc["Batch Processor<br/>Batch size: 8"]
        end
        
        InputValidator --> QueueManager
        QueueManager --> HistoryBuffer
        HistoryBuffer --> PromptBuilder
        PromptBuilder --> TokenGen
        TokenGen --> Sampler
        Sampler --> BatchProc
    end
    
    subgraph GPU["GPU Allocation"]
        GPU1["<<device>> GPU1<br/>Llama Part 1<br/>Layers 0-10<br/>16GB VRAM"]
        GPU2["<<device>> GPU2<br/>Llama Part 2<br/>Layers 11-21<br/>16GB VRAM"]
        GPU3["<<device>> GPU3<br/>Llama Part 3<br/>Layers 22-32<br/>16GB VRAM"]
        
        GPU1 -.pipeline.-> GPU2
        GPU2 -.pipeline.-> GPU3
    end
    
    subgraph Memory["Memory Hierarchy"]
        VRAM["VRAM: 48GB<br/>Model: 32GB<br/>KV Cache: 12GB<br/>Activation: 4GB"]
        
        SystemRAM["System RAM: 128GB<br/>Context: 32GB<br/>Queue: 8GB<br/>OS: 88GB"]
    end
    
    InferenceEngine -.executes on.-> GPU1
    InferenceEngine -.executes on.-> GPU2
    InferenceEngine -.executes on.-> GPU3
    
    ContextEngine -.uses.-> SystemRAM
    InferenceEngine -.uses.-> VRAM
    
    style LLMService fill:#fff9c4,stroke:#f57f17,stroke-width:3px
    style RequestHandler fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style ContextEngine fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    style InferenceEngine fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    style GPU fill:#ffebee,stroke:#c62828,stroke-width:2px
    style Memory fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
