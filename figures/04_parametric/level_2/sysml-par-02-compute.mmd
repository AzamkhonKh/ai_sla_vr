---
title: Compute Constraints (par)
---
graph TB
    subgraph LLMSelection["«constraint def» LLM Model Selection"]
        ModelChoice["Selected: GPT-4o (128K context)"]
        AlternativeA["Alternative A: Claude 3.5 Sonnet"]
        AlternativeB["Alternative B: Llama 3.1 70B (self-hosted)"]
        
        ModelChoice -->|primary choice| Rationale["Rationale:<br/>- Multilingual: EN/DE/ES<br/>- Low latency: 800-1200ms<br/>- High quality: >95% relevance<br/>- Cost: $0.0025/turn"]
        
        AlternativeA -.fallback.-> Rationale2["Fallback:<br/>- Better reasoning<br/>- 200K context<br/>- $0.003/turn<br/>- Latency: 900-1300ms"]
        
        AlternativeB -.cost-reduction.-> Rationale3["Cost Optimization:<br/>- $0.0008/turn (70% savings)<br/>- Requires GPU: 2×A100<br/>- Latency: 1200-1800ms<br/>- Quality: 90% of GPT-4o"]
    end
    
    subgraph ComputeInfra["«constraint def» Compute Infrastructure"]
        CloudPods["Cloud Pods (Kubernetes)"]
        GPURequirements["GPU Requirements"]
        CPURequirements["CPU Requirements"]
        
        CloudPods --> OnDemand["On-Demand Pods:<br/>- Min: 15 pods<br/>- Max: 50 pods<br/>- CPU: 2 vCPU per pod<br/>- Memory: 4 GB per pod"]
        
        CloudPods --> ReservedInstances["Reserved Capacity:<br/>- Base: 20 pods (1 year)<br/>- Savings: 40%<br/>- Cost: €600/month"]
        
        GPURequirements --> STTAccel["STT Service GPU:<br/>- Model: Whisper Large v3<br/>- GPU: NVIDIA T4 (16GB)<br/>- Replicas: 3-10<br/>- Latency: 200-500ms<br/>- Throughput: 100 req/s"]
        
        GPURequirements --> SelfHostedLLM["Self-Hosted LLM (optional):<br/>- Model: Llama 3.1 70B<br/>- GPU: 2×A100 (80GB each)<br/>- Quantization: INT8<br/>- Latency: 1200-1800ms<br/>- Cost: €800/month (vs €1200 API)"]
        
        CPURequirements --> StandardServices["Standard Services:<br/>- API Gateway: 2 vCPU, 4GB<br/>- Conversation: 4 vCPU, 8GB<br/>- TTS: 2 vCPU, 4GB<br/>- Session/User: 1 vCPU, 2GB"]
    end
    
    subgraph SLADefinition["«constraint def» Service Level Agreement (SLA)"]
        Availability["Availability: 99.5%"]
        Latency["Latency P99: ≤1.5s"]
        ErrorRate["Error Rate: ≤1%"]
        DataRetention["Data Retention: 12 months"]
        
        Availability --> UptimeTarget["Monthly Uptime:<br/>- Target: 99.5%<br/>- Downtime: 3.6 hours/month<br/>- Planned maintenance: 2h/month<br/>- Unplanned: <1.6h/month"]
        
        Availability --> Redundancy["Redundancy:<br/>- Multi-zone: 3 AZs<br/>- Database: Primary + 2 replicas<br/>- Cache: 6-node cluster<br/>- Load balancer: 2 instances"]
        
        Latency --> BreakdownP99["P99 Latency Breakdown:<br/>- Network: 50-100ms<br/>- Auth/Session: <50ms<br/>- STT: 200-500ms<br/>- LLM: 800-1200ms<br/>- TTS: 200-300ms<br/>- Total: 1.25-1.45s (target: <1.5s)"]
        
        Latency --> Monitoring["Monitoring:<br/>- Prometheus: 15s scrape<br/>- Grafana: Real-time dashboards<br/>- Jaeger: 10% trace sampling<br/>- Alerts: <2s p99 (SEV2), >3s (SEV1)"]
        
        ErrorRate --> ErrorTypes["Error Budget:<br/>- 5xx errors: <0.5%<br/>- 4xx errors: <0.3%<br/>- Timeout errors: <0.2%<br/>- Monthly budget: 99% success rate"]
        
        DataRetention --> BackupPolicy["Backup Policy:<br/>- PostgreSQL: Daily full + WAL<br/>- Redis: RDB snapshots (1h)<br/>- Milvus: Weekly full<br/>- Retention: 12 months<br/>- Recovery: RPO 1h, RTO 4h"]
    end
    
    subgraph CostModel["«constraint def» Cost Model (per 10K users)"]
        TotalCost["Total: €1,800/month"]
        
        TotalCost --> ComputeCost["Compute: €800/month<br/>- Reserved pods: €600<br/>- On-demand burst: €200"]
        
        TotalCost --> LLMCost["LLM API: €500/month<br/>- GPT-4o: 200K turns/month<br/>- $0.0025 per turn<br/>- 10 turns per session<br/>- 20K sessions/month"]
        
        TotalCost --> StorageCost["Storage: €300/month<br/>- PostgreSQL: €150 (100GB SSD)<br/>- Redis: €100 (96GB memory)<br/>- Milvus: €50 (50GB vectors)"]
        
        TotalCost --> NetworkCost["Network: €200/month<br/>- Data transfer: 5TB<br/>- Audio: 3TB (TTS/STT)<br/>- API: 2TB (JSON/gRPC)"]
    end
    
    subgraph ScalingStrategy["«constraint» Auto-Scaling Strategy"]
        HPA["Horizontal Pod Autoscaler"]
        VPA["Vertical Pod Autoscaler"]
        
        HPA --> CPUBased["CPU-based scaling:<br/>- Target: 70% CPU utilization<br/>- Scale up: +2 pods every 30s<br/>- Scale down: -1 pod every 5min<br/>- Min replicas: 3-5 per service"]
        
        HPA --> QueueBased["Queue-based scaling:<br/>- Redis queue depth<br/>- Target: <100 messages<br/>- Scale up: depth >200<br/>- Scale down: depth <50"]
        
        VPA --> MemoryOptimization["Memory optimization:<br/>- Monitor OOM kills<br/>- Adjust requests: 80% actual<br/>- Adjust limits: 120% actual<br/>- Update: weekly"]
    end
    
    ModelChoice ==>|determines| LLMCost
    GPURequirements ==>|impacts| ComputeCost
    Availability ==>|constrains| Redundancy
    Latency ==>|constrains| BreakdownP99
    TotalCost ==>|budget limit| ModelChoice
    HPA ==>|optimizes| ComputeCost
    
    style LLMSelection fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    style ComputeInfra fill:#e3f2fd,stroke:#1565c0,stroke-width:3px
    style SLADefinition fill:#fff3e0,stroke:#ef6c00,stroke-width:3px
    style CostModel fill:#ffebee,stroke:#c62828,stroke-width:3px
    style ScalingStrategy fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
