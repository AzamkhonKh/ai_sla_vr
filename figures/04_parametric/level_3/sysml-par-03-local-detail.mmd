graph TB
    subgraph LLMSelection["«constraint def» Open-Source LLM Selection (Local Deployment)"]
        PrimaryModel["Primary: Llama 3.1 70B Instruct"]
        FallbackModel["Fallback: Mistral 7B Instruct v0.3"]
        LightweightModel["Lightweight: Phi-3 Mini 3.8B"]
        
        PrimaryModel --> Specs1["Specs:<br/>- Parameters: 70B<br/>- Context: 128K tokens<br/>- Quantization: INT8 (FP16→INT8)<br/>- Model size: 70GB (FP16), 35GB (INT8)<br/>- FLOPS (inference): ~140 TFLOPS<br/>- Latency: 800-1200ms (batch=1)<br/>- Quality: MMLU 79.3%, multilingual"]
        
        FallbackModel --> Specs2["Specs:<br/>- Parameters: 7B<br/>- Context: 32K tokens<br/>- Quantization: INT8<br/>- Model size: 7GB (INT8)<br/>- FLOPS (inference): ~14 TFLOPS<br/>- Latency: 200-400ms (batch=1)<br/>- Quality: MMLU 60.5%"]
        
        LightweightModel --> Specs3["Specs:<br/>- Parameters: 3.8B<br/>- Context: 128K tokens<br/>- Quantization: INT4<br/>- Model size: 2.3GB (INT4)<br/>- FLOPS (inference): ~7.6 TFLOPS<br/>- Latency: 100-200ms (batch=1)<br/>- Quality: MMLU 69.9%"]
    end
    
    subgraph ComputeHardware["«constraint def» Hardware Requirements (Per Service)"]
        LLMHardware["LLM Inference Server"]
        STTHardware["STT Service (Whisper)"]
        TTSHardware["TTS Service (Piper)"]
        StandardServices["Standard Services"]
        
        LLMHardware --> LLMSpecs["GPU: 2×NVIDIA A100 80GB<br/>- Compute: 312 TFLOPS (FP16), 624 TFLOPS (INT8)<br/>- Memory bandwidth: 2TB/s per GPU<br/>- VRAM: 80GB per GPU (160GB total)<br/>- NVLink: 600 GB/s inter-GPU<br/>- Power: 400W per GPU (800W total)<br/><br/>CPU: 32 cores (AMD EPYC 7763)<br/>- Base clock: 2.45 GHz<br/>- Boost: 3.5 GHz<br/>- Compute: ~3.5 TFLOPS (FP64)<br/><br/>RAM: 256GB DDR4-3200<br/>- Bandwidth: 204.8 GB/s<br/>- Latency: ~60ns<br/><br/>Storage: 2TB NVMe SSD<br/>- Read: 7000 MB/s<br/>- Write: 5000 MB/s"]
        
        STTHardware --> STTSpecs["GPU: NVIDIA T4 16GB (per replica)<br/>- Compute: 65 TFLOPS (INT8), 8.1 TFLOPS (FP16)<br/>- Memory bandwidth: 320 GB/s<br/>- VRAM: 16GB GDDR6<br/>- Power: 70W<br/>- Replicas: 3-10<br/><br/>CPU: 8 cores<br/>- Clock: 2.5 GHz<br/>- Compute: ~640 GFLOPS (FP64)<br/><br/>RAM: 32GB DDR4<br/>- Bandwidth: 51.2 GB/s<br/><br/>Model: Whisper Large v3<br/>- Parameters: 1.55B<br/>- FLOPS (inference): ~31 TFLOPS<br/>- Latency: 200-500ms per 30s audio"]
        
        TTSHardware --> TTSSpecs["CPU: 4 cores (per replica)<br/>- Clock: 3.0 GHz<br/>- Compute: ~384 GFLOPS (FP64)<br/>- Replicas: 3-10<br/><br/>RAM: 8GB DDR4<br/>- Bandwidth: 25.6 GB/s<br/><br/>Model: Piper TTS<br/>- Voices: thorsten-low (DE), amy-low (EN)<br/>- Model size: 50MB per voice<br/>- FLOPS (synthesis): ~2 GFLOPS<br/>- Latency: 200-300ms per sentence"]
        
        StandardServices --> StandardSpecs["API Gateway, Session, User, Analytics:<br/><br/>CPU: 2 cores per pod<br/>- Clock: 2.8 GHz<br/>- Compute: ~180 GFLOPS (FP64)<br/><br/>RAM: 4GB DDR4<br/>- Bandwidth: 12.8 GB/s<br/><br/>Storage: 50GB SSD<br/>- IOPS: 3000 (random 4K)"]
    end
    
    subgraph FLOPSBudget["«constraint def» FLOPS Budget & Throughput"]
        TotalFLOPS["Total Cluster Compute"]
        PerRequest["Per Request FLOPS"]
        Throughput["System Throughput"]
        
        TotalFLOPS --> ClusterFLOPS["GPU Compute:<br/>- LLM: 624 TFLOPS (2×A100 INT8)<br/>- STT: 195-650 TFLOPS (3-10×T4 INT8)<br/>- Total GPU: 819-1274 TFLOPS<br/><br/>CPU Compute:<br/>- LLM server: 3.5 TFLOPS<br/>- STT pods: 1.9-6.4 TFLOPS (3-10 pods)<br/>- TTS pods: 1.2-3.8 TFLOPS (3-10 pods)<br/>- Standard services: 3.6-9.0 TFLOPS (20-50 pods)<br/>- Total CPU: 10.2-22.7 TFLOPS<br/><br/>Grand Total: 829-1297 TFLOPS"]
        
        PerRequest --> RequestFLOPS["Single Conversation Turn:<br/>- STT (30s audio): ~31 TFLOPS<br/>- LLM (200 tokens): ~140 TFLOPS<br/>- TTS (50 tokens): ~2 GFLOPS<br/>- Total per turn: ~171 TFLOPS<br/>- Time: 1.2-1.5s<br/>- Utilization: 114-143 TFLOPS/s"]
        
        Throughput --> SystemThroughput["Max Concurrent Requests:<br/>- GPU bottleneck: 624 TFLOPS ÷ 140 TFLOPS<br/>- Max parallel: ~4 LLM requests<br/>- With batching: 8-12 requests<br/>- Peak throughput: 6-10 turns/s<br/>- Daily capacity: 518K-864K turns"]
    end
    
    subgraph MemoryConstraints["«constraint def» Memory & Bandwidth"]
        VRAMUsage["GPU VRAM Usage"]
        RAMUsage["System RAM Usage"]
        Bandwidth["Memory Bandwidth"]
        
        VRAMUsage --> VRAMBreakdown["LLM Server (2×A100, 160GB total):<br/>- Model weights: 35GB (Llama 70B INT8)<br/>- KV cache: 50GB (128K context, batch=4)<br/>- Activations: 20GB (intermediate)<br/>- Reserved: 10GB (system)<br/>- Total: 115GB / 160GB (72% util)<br/><br/>STT Server (T4, 16GB per replica):<br/>- Model weights: 3.1GB (Whisper Large v3)<br/>- Audio buffer: 2GB (30s × 16kHz × 32-bit)<br/>- Activations: 5GB<br/>- Reserved: 2GB<br/>- Total: 12.1GB / 16GB (76% util)"]
        
        RAMUsage --> RAMBreakdown["LLM Server (256GB DDR4):<br/>- OS + drivers: 16GB<br/>- Model loading buffer: 70GB<br/>- Request queue: 20GB<br/>- Tokenization: 10GB<br/>- Application: 30GB<br/>- Cache: 80GB<br/>- Available: 30GB (12% free)<br/><br/>STT Pod (32GB per replica):<br/>- OS: 4GB<br/>- Model cache: 8GB<br/>- Audio processing: 12GB<br/>- Available: 8GB (25% free)"]
        
        Bandwidth --> BandwidthReq["Critical Paths:<br/>- GPU↔GPU (NVLink): 600 GB/s required<br/>- GPU↔RAM (PCIe 4.0 x16): 64 GB/s available<br/>- Model loading: 35GB ÷ 64 GB/s = 546ms<br/>- RAM→CPU: 204.8 GB/s (DDR4-3200)<br/>- Storage→RAM: 7 GB/s (NVMe)<br/><br/>Bottleneck: PCIe bandwidth (64 GB/s)<br/>- Token generation: 200 tokens × 70B × 1B/token<br/>- Data transfer: 14GB per forward pass<br/>- Time: 14GB ÷ 64 GB/s = 219ms overhead"]
    end
    
    subgraph PowerThermal["«constraint» Power & Cooling"]
        PowerBudget["Power Consumption"]
        Cooling["Cooling Requirements"]
        
        PowerBudget --> PowerBreakdown["Per Server Rack:<br/><br/>LLM Server:<br/>- 2×A100 GPU: 800W<br/>- CPU (EPYC): 280W<br/>- RAM: 40W<br/>- Storage + NIC: 50W<br/>- PSU efficiency: 95%<br/>- Total: 1232W @ wall<br/><br/>3×STT Servers:<br/>- 3×T4 GPU: 210W<br/>- 3×CPU: 180W<br/>- RAM + storage: 60W<br/>- Total: 473W @ wall<br/><br/>10×Standard Pods:<br/>- Total: 500W @ wall<br/><br/>Networking + overhead: 300W<br/><br/>Grand Total: 2505W per rack<br/>Daily: 60.1 kWh<br/>Monthly: ~1800 kWh<br/>Cost @ €0.15/kWh: €270/month"]
        
        Cooling --> CoolingReq["Heat Dissipation:<br/>- Total heat: 2505W = 8549 BTU/h<br/>- Cooling capacity: 1 ton (12000 BTU/h)<br/>- Air cooling: 2×120mm fans per GPU<br/>- Ambient temp: <28°C<br/>- GPU temp: 65-75°C (target)<br/>- CPU temp: 60-70°C (target)<br/><br/>Datacenter Requirements:<br/>- PUE: 1.3 (industry average)<br/>- Total power: 2505W × 1.3 = 3257W<br/>- Cooling load: 752W dedicated"]
    end
    
    subgraph CostModel["«constraint» Cost Model (Local Deployment)"]
        HardwareCost["Hardware CapEx"]
        OperatingCost["Operating Cost (Monthly)"]
        
        HardwareCost --> CapEx["Initial Investment:<br/>- 2×NVIDIA A100 80GB: €24,000<br/>- Server (CPU, RAM, storage): €8,000<br/>- 3×NVIDIA T4 16GB: €6,000<br/>- 3×STT servers: €12,000<br/>- 10×Standard servers: €20,000<br/>- Networking (10GbE): €3,000<br/>- Rack + PDU: €2,000<br/>- Total CapEx: €75,000<br/>- Amortization (3 years): €2,083/month"]
        
        OperatingCost --> OpEx["Monthly Operating Cost:<br/>- Electricity: €270 (1800 kWh)<br/>- Cooling: €100 (additional power)<br/>- Internet (1Gbps): €150<br/>- Colocation/space: €500<br/>- Maintenance: €200<br/>- Total OpEx: €1,220/month<br/><br/>Total Monthly (CapEx + OpEx): €3,303<br/>Cost per 10K users: €0.33/user/month<br/>Cost per turn: €0.0008 (vs €0.0025 cloud API)"]
    end
    
    PrimaryModel ==>|determines| LLMHardware
    LLMHardware ==>|constrains| VRAMUsage
    VRAMUsage ==>|limits| Throughput
    TotalFLOPS ==>|enables| SystemThroughput
    PowerBudget ==>|requires| Cooling
    HardwareCost ==>|enables| OperatingCost
    
    style LLMSelection fill:#e8f5e9,stroke:#2e7d32,stroke-width:3px
    style ComputeHardware fill:#e3f2fd,stroke:#1565c0,stroke-width:3px
    style FLOPSBudget fill:#fff3e0,stroke:#ef6c00,stroke-width:3px
    style MemoryConstraints fill:#ffebee,stroke:#c62828,stroke-width:3px
    style PowerThermal fill:#f3e5f5,stroke:#6a1b9a,stroke-width:3px
    style CostModel fill:#e0f2f1,stroke:#004d40,stroke-width:3px
